<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>In Browser RAG + LLM with WebGPU Acceleration</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            background-color: #f0f0f0;
        }
        #app {
            background: #fff;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        h1 {
            font-size: 24px;
            margin-bottom: 20px;
        }
        textarea {
            width: 100%;
            height: 100px;
            margin-bottom: 10px;
        }
        button {
            display: block;
            width: 100%;
            padding: 10px;
            background: #007BFF;
            color: #fff;
            border: none;
            border-radius: 5px;
            cursor: pointer;
        }
        button:hover {
            background: #0056b3;
        }
    </style>
</head>
<body>
    <div id="app">
        <h1>In Browser RAG + LLM with WebGPU Acceleration</h1>
        <label id="init-label">Initializing...</label>
        <textarea id="prompt-textarea" placeholder="Enter your prompt here"></textarea>
        <button id="submit-button">Submit</button>
        <textarea id="response-textarea" placeholder="Response will appear here" readonly></textarea>
    </div>
    <script type="module">
        import * as webllm from "https://esm.run/@mlc-ai/web-llm";

        function setLabel(id, text) {
            const label = document.getElementById(id);
            if (label == null) {
                throw new Error("Cannot find label " + id);
            }
            label.innerText = text;
        }

        async function main() {
            const initProgressCallback = (report) => {
                setLabel("init-label", report.text);
            };

            //const selectedModel = "Qwen2-0.5B-Instruct-q0f16-MLC"; //About 1G of vram
            const selectedModel = "Llama-3-8B-Instruct-q4f32_1-MLC"; //About 5G of vram

            const engine = await webllm.CreateMLCEngine(
                selectedModel,
                {
                    initProgressCallback: initProgressCallback,
                    logLevel: "INFO",
                },
                {
                    context_window_size: 2048,
                }
            );

            const promptTextarea = document.getElementById("prompt-textarea");
            const submitButton = document.getElementById("submit-button");
            const responseTextarea = document.getElementById("response-textarea");

            const handleSubmit = async () => {
                const prompt = promptTextarea.value;

                try {
                    const reply = await engine.chat.completions.create({
                        messages: [{ role: "user", content: prompt }],
                        n: 1,
                        temperature: 1.5,
                        max_tokens: 256,
                    });

                    if (reply && reply.choices && reply.choices[0] && reply.choices[0].message) {
                        responseTextarea.value = reply.choices[0].message.content || '';
                    } else {
                        console.error('Reply is not in the expected format');
                    }
                    console.log(reply.usage);
                } catch (error) {
                    console.error('Error fetching model response:', error);
                    responseTextarea.value = 'Error fetching model response. Check console for details.';
                }
            };

            submitButton.addEventListener("click", handleSubmit);
        }

        main();
    </script>
</body>
</html>

